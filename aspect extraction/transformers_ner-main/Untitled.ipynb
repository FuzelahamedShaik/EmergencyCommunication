{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858b44f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83751fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "     -------------------------------------- 469.0/469.0 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     -------------------------------------- 323.6/323.6 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-win_amd64.whl (20.6 MB)\n",
      "     ---------------------------------------- 20.6/20.6 MB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.9/132.9 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f3c046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==1.11.0\n",
      "  Using cached datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "Collecting pytorch-crf==0.7.2\n",
      "  Using cached pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
      "Collecting torch==1.9.0\n",
      "  Using cached torch-1.9.0-cp39-cp39-win_amd64.whl (222.0 MB)\n",
      "Collecting torchvision==0.10.0\n",
      "  Using cached torchvision-0.10.0-cp39-cp39-win_amd64.whl (920 kB)\n",
      "Collecting transformers==4.6.1\n",
      "  Using cached transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: xxhash in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (1.21.5)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (2022.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: dill in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (0.3.6)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Using cached huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: packaging in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets==1.11.0->-r requirements.txt (line 1)) (11.0.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from torch==1.9.0->-r requirements.txt (line 3)) (4.3.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from torchvision==0.10.0->-r requirements.txt (line 4)) (9.2.0)\n",
      "  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from transformers==4.6.1->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from transformers==4.6.1->-r requirements.txt (line 5)) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from transformers==4.6.1->-r requirements.txt (line 5)) (2022.7.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 1)) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 1)) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from tqdm>=4.42->datasets==1.11.0->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from packaging->datasets==1.11.0->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 1)) (2022.1)\n",
      "Requirement already satisfied: six in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 5)) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 5)) (1.1.0)\n",
      "Installing collected packages: torch, pytorch-crf, torchvision, sacremoses, huggingface-hub, transformers, datasets\n",
      "Successfully installed datasets-1.11.0 huggingface-hub-0.0.8 pytorch-crf-0.7.2 sacremoses-0.0.53 torch-1.9.0 torchvision-0.10.0 transformers-4.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe and convert-onnx-to-caffe2.exe are installed in 'C:\\Users\\Fuzel Shaik\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script sacremoses.exe is installed in 'C:\\Users\\Fuzel Shaik\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\Fuzel Shaik\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\Fuzel Shaik\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\Fuzel Shaik\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyabsa 2.0.28 requires transformers>=4.18.0, but you have transformers 4.6.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eccc5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aec3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import  TokenClassifierOutput\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertLstmCRF(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, dropout=config.dropout, batch_first=True,\n",
    "                              bidirectional=True)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        lstm_output, hc = self.bilstm(sequence_output)\n",
    "        logits = self.classifier(lstm_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n",
    "            loss = 0 - log_likelihood\n",
    "        else:\n",
    "            tags = self.crf.decode(logits)\n",
    "        tags = torch.Tensor(tags)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (tags,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return loss, tags\n",
    "\n",
    "class BertCRF(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n",
    "            loss = 0 - log_likelihood\n",
    "        else:\n",
    "            tags = self.crf.decode(logits)\n",
    "        tags = torch.Tensor(tags)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (tags,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return loss, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d89a650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\fuzel shaik\\appdata\\roaming\\python\\python39\\site-packages (1.11.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Using cached huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fuzel shaik\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: huggingface-hub, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.11.0\n",
      "    Uninstalling datasets-1.11.0:\n",
      "      Successfully uninstalled datasets-1.11.0\n",
      "Successfully installed datasets-2.10.1 huggingface-hub-0.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.6.1 requires huggingface-hub==0.0.8, but you have huggingface-hub 0.12.1 which is incompatible.\n",
      "pyabsa 2.0.28 requires transformers>=4.18.0, but you have transformers 4.6.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c74c3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to C:\\Users\\Fuzel Shaik\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find file at https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6248\\991659393.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertCRF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conll2003'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[1;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\conll2003\\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\\conll2003.py\u001b[0m in \u001b[0;36m_split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;34m\"test\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"{_URL}{_TEST_FILE}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         }\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mdownloaded_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls_to_download\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         return [\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\utils\\download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\utils\\download_manager.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, url_or_urls)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\utils\\py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\utils\\py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\utils\\py_utils.py\u001b[0m in \u001b[0;36m_single_map_nested\u001b[1;34m(args)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\utils\\download_manager.py\u001b[0m in \u001b[0;36m_download\u001b[1;34m(self, url_or_filename, download_config)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\utils\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\datasets\\utils\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find file at https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from transformers import BertTokenizerFast, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "\n",
    "from models import BertCRF\n",
    "\n",
    "train_dataset, test_dataset = load_dataset('conll2003', split=['train', 'test'])\n",
    "print(train_dataset, test_dataset)\n",
    "\n",
    "model = BertCRF.from_pretrained('bert-base-cased', num_labels=9)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    result = {\n",
    "        'label_ids': [],\n",
    "        'input_ids': [],\n",
    "        'token_type_ids': [],\n",
    "    }\n",
    "    max_length = tokenizer.max_model_input_sizes['bert-base-cased']\n",
    "\n",
    "    for tokens, label in zip(batch['tokens'], batch['label_ids']):\n",
    "        tokenids = tokenizer(tokens, add_special_tokens=False)\n",
    "\n",
    "        token_ids = []\n",
    "        label_ids = []\n",
    "        for ids, lab in zip(tokenids['input_ids'], label):\n",
    "            if len(ids) > 1 and lab % 2 == 1:\n",
    "                token_ids.extend(ids)\n",
    "                chunk = [lab + 1] * len(ids)\n",
    "                chunk[0] = lab\n",
    "                label_ids.extend(chunk)\n",
    "            else:\n",
    "                token_ids.extend(ids)\n",
    "                chunk = [lab] * len(ids)\n",
    "                label_ids.extend(chunk)\n",
    "\n",
    "        token_type_ids = tokenizer.create_token_type_ids_from_sequences(token_ids)\n",
    "        token_ids = tokenizer.build_inputs_with_special_tokens(token_ids)\n",
    "        label_ids.insert(0, 0)\n",
    "        label_ids.append(0)\n",
    "        result['input_ids'].append(token_ids)\n",
    "        result['label_ids'].append(label_ids)\n",
    "        result['token_type_ids'].append(token_type_ids)\n",
    "\n",
    "    result = tokenizer.pad(result, padding='longest', max_length=max_length, return_attention_mask=True)\n",
    "    for i in range(len(result['input_ids'])):\n",
    "        diff = len(result['input_ids'][i]) - len(result['label_ids'][i])\n",
    "        result['label_ids'][i] += [0] * diff\n",
    "    return result\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(['id', 'pos_tags', 'chunk_tags'])\n",
    "train_dataset = train_dataset.rename_column('ner_tags', 'label_ids')\n",
    "test_dataset = test_dataset.remove_columns(['id', 'pos_tags', 'chunk_tags'])\n",
    "test_dataset = test_dataset.rename_column('ner_tags', 'label_ids')\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label_ids'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label_ids'])\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.flatten()\n",
    "    preds = pred.predictions.flatten()\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    print(classification_report(labels, preds))\n",
    "    return {\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3b19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
